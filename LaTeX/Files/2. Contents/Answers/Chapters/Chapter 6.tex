\setcounter{section}{6}
\setcounter{subsection}{1}
\setcounter{answer}{0}

\subsection{Chapter 6: Comparing more than two means}

\answer{
    \vspace*{-10pt}
    \answercode{{\color{dataset}\# Be sure to set your working directory when providing a relative path} \\
dataset8 <- read.csv(\textquotesingle eyeColor.csv\textquotesingle) \\
\\
ttestData  <- subset(dataset8,\\ 
                     \hspace*{110pt}dataset8\$Group == \textquotesingle Blue\textquotesingle | \\
                     \hspace*{110pt}dataset8\$Group == \textquotesingle Brown\textquotesingle)
}
}

\answer{
    $H_0$: $\mu_1 = \mu_2$ \hspace*{4cm} $H_1$: $\mu_1 \neq \mu_2$
}

\answer{
    \vspace*{-10pt}
    \answercode{blue <- subset(ttestData\$Score, ttestData\$Group == \textquotesingle Blue\textquotesingle) \\
brown <- subset(ttestData\$Score, ttestData\$Group == \textquotesingle Brown\textquotesingle) \\
t.test(blue, brown, var.equal = TRUE) {\color{dataset}\# p-value: 0.1401}}
}

\answer{
    The p-value is 0.1401, which is higher than the 0.05 required to reject $H_0$. $H_0$ is not rejected with 95\% confidence. You can be 95\% confident that the mean of the blue group is the same as the mean of the brown group. There is a risk of a type-II error.
}

\answer{
    The content of \rcode{dummyBrown} is a 0 for blue eyes, and a 1 for brown eyes. This kind of variable is called a dummy variable. \\
    \\
    \answercode{dummyBrown <- as.numeric(ttestData\$Group == \textquotesingle Brown\textquotesingle) \\
ttestData <- cbind(ttestData, dummyBrown)
}
}

\answer{
    \vspace*{-10pt}
    \answercode{ttestreg <- lm(formula = Score {\raise.17ex\hbox{$\scriptstyle\sim$}} dummyBrown, data = ttestData)}
}

\answer{
    \vspace*{-10pt}
    \answercode{summary(ttestreg) {\color{dataset}\# p-value dummyBrown: 0.14}}
}

\answerbreakline \setcounter{subsection}{2} \setcounter{answer}{0}

\answer{
    $H_0$: $\mu_{1} = \mu_{2} = \mu_{3} = \mu_{4}$ \hspace*{1cm} $H_1$: $\mu_{1} \neq \mu_{2} \neq \mu_{3} \neq \mu_{4}$
}

\answer{
    $df_M = k - 1 = 4 - 1 = 3$ \hspace{1.2cm} $df_R = n - k = 222 - 4 = 218$
}

\clearpage % Page break

\answer{
    Critical F-value: 2.646 \\
    \\
    \answercode{df1 <- 4 - 1                {\color{dataset}\# 3} \\
df2 <- nrow(dataset8) - 4   {\color{dataset}\# 218} \\
qf(p = 0.95, df1 = df1, df2 = df2) {\color{dataset}\# 2.646}}
}

\answer{
    \vspace*{-10pt}
    \answercode{anovaResult <- aov(formula = Score {\raise.17ex\hbox{$\scriptstyle\sim$}} Group, data = dataset8)}
}

\answer{
    F-value: 2.894 \hspace{4cm} p-value: 0.0362 \\ 
    \\
    \answercode{summary(anovaResult) \\
{\color{dataset}\# F-value: 2.894} \\
{\color{dataset}\# p-value: 0.0362}
}
}

\answer{
    The p-value is 0.0362, which is lower than the 0.05 required to reject $H_0$. The sample F-value is 2.894, which is higher than the critical F-value of 2.646. $H_0$ is rejected with 95\% confidence. The means of the four groups are not equal to each other. There is a risk of 5\% for a type-I error.
}

\answerbreakline \setcounter{subsection}{3} \setcounter{answer}{0}

\answer{
    \vspace*{-10pt}
    \answercode{dummyBrown <- as.numeric(dataset8\$Group == \textquotesingle Brown\textquotesingle) {\color{dataset}\# Brown eyes} \\
dataset8 <- cbind(dataset8, dummyBrown) \\
 \\
dummyBlue <- as.numeric(dataset8\$Group == \textquotesingle Blue\textquotesingle)   {\color{dataset}\# Blue eyes} \\
dataset8 <- cbind(dataset8, dummyBlue) \\
 \\
dummyGreen <- as.numeric(dataset8\$Group == \textquotesingle Green\textquotesingle) {\color{dataset}\# Green eyes} \\
dataset8 <- cbind(dataset8, dummyGreen)}
}

\answer{
    \vspace*{-10pt}
    anovaReg <- lm(formula = Score {\raise.17ex\hbox{$\scriptstyle\sim$}} dummyBrown + dummyBlue + dummyGreen, data = dataset8)
}

\answer{
    F-value: 2.894  \hspace*{4cm} p-value: 0.0362 \\
    \\
        \answercode{summary(anovaReg) \\
{\color{dataset}\# F-value: 2.894} \\
{\color{dataset}\# p-value: 0.0362} \\
{\color{dataset}\# R-squared: 0.0383}
}
}

\answer{
    Yes, the results are the same.
}

\answerbreakline \setcounter{subsection}{4} \setcounter{answer}{0}

\clearpage % Page break

\answer{
    \vspace*{-10pt}
    \answercode{ancovaReg <- lm(formula = Score {\raise.17ex\hbox{$\scriptstyle\sim$}} dummyBrown + dummyBlue + \\
    \hspace*{190pt}dummyGreen + initialScore, \\
                \hspace*{90pt}data = dataset8)}
}

\answer{
    F-value: 2.252		\hspace*{4cm}		p-value: 0.064 \\
    \\
    \answercode{summary(ancovaReg) \\
{\color{dataset}\# F-value: 2.252} \\ 
{\color{dataset}\# p-value: 0.064} \\
{\color{dataset}\# R-squared: 0.0398}
}
}

\answer{
    The p-value is 0.0645, which is higher than the 0.05 required to reject $H_0$. $H_0$ is not rejected with 95\% confidence. The means are equal to each other if you consider the initial score as a covariate. There is a 5\% change of a type-I error. 
}

\answer{
    The p-value is of the coefficient of initial score is 0.5539, which means that it is not significantly different from zero. This means that the initial score is not a good predictor of the actual score.
}

\answer{
    $R^2$ \rcode{anovaReg}: 0.0383	\hspace*{4cm}		$R^2$ \rcode{ancovaReg}: 0.0398 \\
    \\
    The \rcode{ancovaReg} regression model explains more variation in the outcome variable score.
}

\answer{
    The groups, together with the initial score, explain 3.98\% of the variance in the dependent variable score.
}

\answer{
    AIC \rcode{anovaReg}: 865.54 \hspace*{4cm}	AIC \rcode{ancovaReg}: 867.18 \\
    \\
    \answercode{AIC(anovaReg)   {\color{dataset}\# AIC: 865.54} \\
AIC(ancovaReg)  {\color{dataset}\# AIC: 867.18}}
}

\answer{
    The AIC value of the \rcode{anovaReg} regression model is the lowest, which means that the \rcode{anovaReg} model fits the data better than the \rcode{ancovaReg} regression model. This means that the model without the covariate is a better model. You may have already seen this, since the covariate in the \rcode{ancovaReg} model was not a good predictor of the score.
}

\answerbreakline \setcounter{subsection}{5} \setcounter{answer}{0}

\clearpage % Page break

\answer{
    \vspace*{-10pt}
    \answercode{{\color{dataset}\# # Be sure to set your working directory when providing a relative path} \\
    load(\textquotesingle iowa.RData\textquotesingle)}
}

\answer{
    The \rcode{iowa} data consists of payments made by the state of Iowa. Payments are assigned to fiscal years that run from July 1 through June 30, and are numbered for the calendar year in which they end. The fiscal year is divided into fiscal periods with 1 being July and 12 being June. The fiscal year also includes a hold-over period for payments made after year end for good and services received on or before June 30.
}

\answer{
    Rows: 12279009 \hspace{4cm} Columns: 22 \\
    \\
    \answercode{nrow(iowa) {\color{dataset}\# 12279009 rows} \\
ncol(iowa) {\color{dataset}\# 22 columns}}
}

\answer{
    Unique services: 8 \\
    \\
    \answercode{unique(iowa\$Service) {\color{dataset}\# 8 unique services} \\
table(iowa\$Service)}
}

\answer{
    Service: Human Services \hspace{4cm} Rows: 6682159
}

\answer{
    Number of rows that show a difference: 2624607 \\
    \\
    \answercode{iowa\$Payment.Issue.Date <- as.Date(iowa\$Payment.Issue.Date,format= \textquotesingle \%m/\%d/\%Y\textquotesingle) \\
iowa\$Invoice.Date <- as.Date(iowa\$Invoice.Date,format = \textquotesingle\%m/\%d/\%Y\textquotesingle) \\
\\
length(which(iowa\$Payment.Issue.Date != iowa\$Invoice.Date)) {\color{dataset}\# 2624607}}
}

\answer{
    \vspace*{-10pt}
    \answercode{dataDif <- data[which(iowa\$Payment.Issue.Date != data\$Invoice.Date),]}
}

\answer{
    \vspace*{-10pt}
    \answercode{dataDif\$dif.days <- dataDif\$Payment.Issue.Date - dataDif\$Invoice.Date \\
    dataDif\$dif.days <- as.numeric(dataDif\$dif.days)}
}

\clearpage % Page break

\answer{
    \vspace*{-7.5pt}
    \begin{minipage}{.5\textwidth}
    Minimum: -3651\\
    Mean: 21.815 \\
    Maximum: 36529 
    \end{minipage}
    \begin{minipage}{.5\textwidth}
    Upper quartile: 33 \\
    Lower quartile: 4 \\
    Standard deviation: 89.24
    \end{minipage} \\
    \\
    \\
    \answercode{min(dataDif\$dif.days) \\
max(dataDif\$dif.days) \\
mean(dataDif\$dif.days) \\
quantile(dataDif\$dif.days) \\
sd(dataDif\$dif.days)}
}

\answer{
    The default histogram does not provide much information due to the fact that \texttt{R} specifies a very wide \textit{x-axis}. \\
    \\
    \answercode{hist(dataDif\$dif.days)}
}

\answer{
    \vspace*{-10pt}
    \answercode{hist(dataDif\$dif.days[dataDif\$dif.days > quantile(dataDif\$dif.days, 0.05) \& \\ 
    \hspace*{30pt}dataDif\$dif.days < quantile(dataDif\$dif.days, 0.95)], breaks = 100)}
}

\answer{
    \vspace*{-10pt}
    \answercode{dataDif2 <- dataDif[(dataDif\$dif.days > (-1)) \& (dataDif\$dif.days <= 365),]}
}

\answer{
    \vspace*{-10pt}
    \answercode{plot(dataDif2\$Amount,dataDif2\$dif.days)}
}

\answer{
    Correlation: -0.0025\\
    \\
    \answercode{cor.test(dataDif2\$Amount,dataDif2\$dif.days)}   
}

\answer{
    The p-value of the correlation test against the value zero is 3.148e-05, which is sufficient enough to reject $H_0$ with 95\% confidence. This implies that there is, with 95\% certainty, a correlation between the the time between invoice and payment, and the amount that is paid.
}

\clearpage % Page break

\answer{
    Administration and regulation: 26.452 \\
    Agriculture and natural resources: 28.275 \\
    Capital: 35.490 \\
    Economic development: 25.815 \\
    Education: 23.868 \\
    Human services: 18.508\\
    Justice system: 25.478 \\
    \\
    \answercode{aggregate(dataDif2\$dif.days, by = list(dataDif2\$Service), FUN = mean)}
}

\answer{
    p-value: < 2e-16 \\
    \\
    \underline{Conclusion}: The p-value is lower than 0.05, so you can reject $H_0$ with 95\% confidence. This means that the means of all expense categories are not equal. There is a 5\% type change of a type-I error. \\
    \\
    \answercode{aovRes <- aov(dif.days {\raise.17ex\hbox{$\scriptstyle\sim$}} Service, data = dataDif2) \\
summary(aovRes)}
}

\answer{
    All means, except for the means of the justice system expenses and the economic development expenses, show a p-value below 5\% and can thus be regarded to differ from each other. \\
    \\
    \answercode{tukeyRes <- TukeyHSD(aovRes)}
}

\answer{
    An ANOVA assumes the dependent variable to be continuous. Some examples of appropriate analyses could be: \\
    \\
    \answercode{{\color{dataset}\# Poisson regression and then interpret the predictors} \\
poissonReg <- glm(dif.days {\raise.17ex\hbox{$\scriptstyle\sim$}} Expense.Category, family = poisson, data = dataDif2) \\
summary(poissonReg) \\
\\
{\color{dataset}\# Kruskal Wallis test and Dunn test to compare individual groups} \\
kruskRes <- kruskal.test(dif.days {\raise.17ex\hbox{$\scriptstyle\sim$}} Service, data = dataDif2) \\
{\color{dataset}\# install.packages(\textquotesingle FSA\textquotesingle); library(FSA)} \\
dunn.res <-dunnTest(dataDif2\$dif.days, dataDif2\$Service)
} 
}

\answer{
    It is a bad idea, the p-value is affected by the number of samples. The higher the sample, the lower the p-value gets. In other words, p-values lose their meaning quite quickly (unless they are non-significant).
}

\clearpage % Page break